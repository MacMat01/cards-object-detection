{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cards Object Detection - YOLOv8",
   "id": "f09c7ca359831703"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Project Overview\n",
    "\n",
    "This Jupyter notebook contains the code for the Strategic Fruits Card Detection project. The goal of this project is to train a YOLOv8 model to detect strategic fruits cards in images. The notebook is divided into several sections, each covering a different aspect of the project."
   ],
   "id": "d21513b6e2a64eb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory. This is the directory from which the script is being run.\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Print the current working directory. This can be useful for debugging, to ensure that your files are being \n",
    "# accessed from the correct location.\n",
    "print(HOME)"
   ],
   "id": "af8aa6c130ebd346",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setting up the Dataset\n",
    "\n",
    "The dataset contains images of strategic fruits cards along with annotations in the form of bounding boxes. The dataset is divided into three parts: train, validation, and test. The images are stored in the `images` directory, and the annotations are stored in the `labels` directory. The annotations are in the YOLO format, which consists of a text file for each image, with each line in the file representing a bounding box in the format `class_id x_center"
   ],
   "id": "e35f0e020b994df8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define train, val, test directories\n",
    "\n",
    "train_images = os.path.join(HOME, \"strategic-fruits-card-detection-dataset/train/images\")\n",
    "train_labels = os.path.join(HOME, \"strategic-fruits-card-detection-dataset/train/labels\")\n",
    "\n",
    "valid_images = os.path.join(HOME, \"strategic-fruits-card-detection-dataset/val/images\")\n",
    "valid_labels = os.path.join(HOME, \"strategic-fruits-card-detection-dataset/val/labels\")\n",
    "\n",
    "test_images = os.path.join(HOME, \"strategic-fruits-card-detection-dataset/test/images\")\n",
    "test_labels = os.path.join(HOME, \"strategic-fruits-card-detection-dataset/test/labels\")\n",
    "\n",
    "yaml_path = os.path.join(HOME, \"strategic-fruits-card-detection-dataset/data.yaml\")"
   ],
   "id": "c3c24002d185cd9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(train_images)\n",
    "print(valid_images)\n",
    "print(test_images)"
   ],
   "id": "87913c82d44287a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "yaml_path",
   "id": "967aaf4ce064edb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the YAML content as a string\n",
    "yaml_content = f\"\"\"\n",
    "train: {os.path.join(HOME, 'strategic-fruits-card-detection-dataset/train/images')}\n",
    "val: {os.path.join(HOME, 'strategic-fruits-card-detection-dataset/val/images')}\n",
    "test: {os.path.join(HOME, 'strategic-fruits-card-detection-dataset/test/images')}\n",
    "\n",
    "nc: 20\n",
    "names: ['1a','2a','3a','4a','5a','1b','2b','3b','4b','5b','1o','2o','3o','4o','5o','1p','2p','3p','4p','5p']\n",
    "\"\"\"\n",
    "\n",
    "# Write the YAML content to a file\n",
    "with open(yaml_path, 'w') as file:\n",
    "    file.write(yaml_content)"
   ],
   "id": "a003a3f8f5c2bd91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the labels\n",
    "classes = ['1a','2a','3a','4a','5a','1b','2b','3b','4b','5b','1o','2o','3o','4o','5o','1p','2p','3p','4p','5p']\n",
    "\n",
    "Idx2Label = {idx: label for idx, label in enumerate(classes)}\n",
    "Label2Index = {label: idx for idx, label in Idx2Label.items()}\n",
    "\n",
    "print('Index to Label Mapping:', Idx2Label)\n",
    "print('Label to Index Mapping:', Label2Index)"
   ],
   "id": "74e8d6e87f854dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Testing the Dataset\n",
    "\n",
    "Let's test the dataset by visualizing some images with annotated bounding boxes.\n",
    "This will help us ensure that the images and labels are loaded correctly and that the annotations are accurate."
   ],
   "id": "fb9b2c1926fa3fff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import cv2  # Run 'pip install opencv-python' if it doesn't work\n",
    "\n",
    "\n",
    "# Function to visualize images with annotated bounding boxes\n",
    "def visualize_image_with_annotation_bboxes(image_dir, label_dir):\n",
    "    \"\"\"\n",
    "    This function visualizes images with annotated bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    image_dir (str): The directory where the images are stored.\n",
    "    label_dir (str): The directory where the label files are stored.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of all the image files in the directory\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "\n",
    "    # Choose 12 random image files from the list\n",
    "    sample_image_files = random.sample(image_files, 12)\n",
    "\n",
    "    # Set up the plot\n",
    "    fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
    "\n",
    "    # Loop over the random images and plot the bounding boxes\n",
    "    for i, image_file in enumerate(sample_image_files):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "\n",
    "        # Load the image\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load the labels for this image\n",
    "        label_path = os.path.join(label_dir, image_file[:-4] + '.txt')\n",
    "        f = open(label_path, 'r')\n",
    "\n",
    "        # Loop over the labels and plot the bounding boxes\n",
    "        for label in f:\n",
    "            class_id, x_center, y_center, width, height = map(float, label.split())\n",
    "            h, w, _ = image.shape\n",
    "            x_min = int((x_center - width / 2) * w)\n",
    "            y_min = int((y_center - height / 2) * h)\n",
    "            x_max = int((x_center + width / 2) * w)\n",
    "            y_max = int((y_center + height / 2) * h)\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cv2.putText(image, Idx2Label[int(class_id)], (x_min, y_min), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1,\n",
    "                        color=(255, 255, 255), thickness=2)\n",
    "\n",
    "        axs[row, col].imshow(image)\n",
    "        axs[row, col].axis('off')\n",
    "\n",
    "    plt.show()  # Display the plot"
   ],
   "id": "1573b158fea9df52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize 6 sample images with bounding boxes\n",
    "\n",
    "visualize_image_with_annotation_bboxes(train_images, train_labels)"
   ],
   "id": "2ad0d8d5ee87b911",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read an image by path\n",
    "image_path = os.path.join(train_images, os.listdir(train_images)[-1])\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Get the size of the image\n",
    "height, width, channels = image.shape\n",
    "print('The image has dimensions {}x{} and {} channels'.format(height, width, channels))"
   ],
   "id": "b1b149087736871a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Model Training\n",
    "\n",
    "In this section, we will train our YOLOv8 model using the training dataset. The model will learn to recognize strategic fruits cards by finding patterns in the training images. We will also monitor the model's performance on the validation dataset during training."
   ],
   "id": "a7ed6f5fb677816c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the CUDA device order environment variable. This environment variable is used by CUDA to determine the order in which it recognizes devices.\n",
    "# When set to \"PCI_BUS_ID\", CUDA will arrange devices in the order they are connected to the PCI bus. This can be useful in multi-GPU setups,\n",
    "# as it ensures a consistent device order based on the hardware configuration rather than the default order used by CUDA.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\""
   ],
   "id": "1c9fe104e0594ce1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ],
   "id": "a4547aed10d39b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "# Load a pretrained nano model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# If multiple GPUs are available, wrap the model with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "    model = DataParallel(model)\n",
    "\n",
    "# Disables the cuDNN backend for PyTorch. cuDNN is a GPU-accelerated library for deep neural networks\n",
    "# provided by NVIDIA, which is used by PyTorch for operations like convolutions. Disabling it can\n",
    "# sometimes help with debugging, but it may also slow down computations. It's generally recommended\n",
    "# to leave this enabled for performance reasons, unless you're encountering specific issues.\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# Free up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Training the model\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=26,\n",
    "    imgsz=max(height, width),  # Image size must be multiple of 32\n",
    "    seed=42,\n",
    "    batch=8,\n",
    "    workers=8,  # Change this according to your system specs (default 4)\n",
    "    patience=5,\n",
    "    name='yolov8n_custom',\n",
    "    device='gpu' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "8668810550ccc4e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# plot the result\n",
    "\n",
    "%matplotlib inline\n",
    "# read in the results.csv file as a pandas dataframe\n",
    "df = pd.read_csv('runs/detect/yolov8n_custom/results.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# ignore FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# create subplots using seaborn\n",
    "fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(15, 15))\n",
    "\n",
    "# plot the columns using seaborn\n",
    "sns.lineplot(x='epoch', y='train/box_loss', data=df, ax=axs[0, 0])\n",
    "sns.lineplot(x='epoch', y='train/cls_loss', data=df, ax=axs[0, 1])\n",
    "sns.lineplot(x='epoch', y='train/dfl_loss', data=df, ax=axs[1, 0])\n",
    "sns.lineplot(x='epoch', y='metrics/precision(B)', data=df, ax=axs[1, 1])\n",
    "sns.lineplot(x='epoch', y='metrics/recall(B)', data=df, ax=axs[2, 0])\n",
    "sns.lineplot(x='epoch', y='metrics/mAP50(B)', data=df, ax=axs[2, 1])\n",
    "sns.lineplot(x='epoch', y='metrics/mAP50-95(B)', data=df, ax=axs[3, 0])\n",
    "sns.lineplot(x='epoch', y='val/box_loss', data=df, ax=axs[3, 1])\n",
    "sns.lineplot(x='epoch', y='val/cls_loss', data=df, ax=axs[4, 0])\n",
    "sns.lineplot(x='epoch', y='val/dfl_loss', data=df, ax=axs[4, 1])\n",
    "\n",
    "# set titles and axis labels for each subplot\n",
    "axs[0, 0].set(title='Train Box Loss')\n",
    "axs[0, 1].set(title='Train Class Loss')\n",
    "axs[1, 0].set(title='Train DFL Loss')\n",
    "axs[1, 1].set(title='Metrics Precision (B)')\n",
    "axs[2, 0].set(title='Metrics Recall (B)')\n",
    "axs[2, 1].set(title='Metrics mAP50 (B)')\n",
    "axs[3, 0].set(title='Metrics mAP50-95 (B)')\n",
    "axs[3, 1].set(title='Validation Box Loss')\n",
    "axs[4, 0].set(title='Validation Class Loss')\n",
    "axs[4, 1].set(title='Validation DFL Loss')\n",
    "\n",
    "# add suptitle and subheader\n",
    "plt.suptitle('Training Metrics and Loss', fontsize=24)\n",
    "\n",
    "# adjust top margin to make space for suptitle\n",
    "plt.subplots_adjust(top=0.8)\n",
    "\n",
    "# adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "207080e7bd1450c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "In this section, we will evaluate the trained model on the test dataset to assess its performance.\n",
    "We will calculate the mean average precision (mAP) at different intersection over union\n",
    "(IoU) thresholds to measure the accuracy of the model's detections.\n",
    "A higher mAP indicates better performance.\n",
    "We will also visualize some of the model's predictions on the test images.\n",
    "This will help us understand how well the model is performing in practice."
   ],
   "id": "1c44a27a1979aa64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Loading the best performing model\n",
    "model = YOLO('runs/detect/yolov8n_custom/weights/best.pt')\n",
    "\n",
    "# Evaluating the model on test dataset\n",
    "metrics = model.val(conf=0.25, split='test')"
   ],
   "id": "70ea8bfb7a49bf1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Mean Average Precision @.5:.95 : {metrics.box.map}\")\n",
    "print(f\"Mean Average Precision @ .50   : {metrics.box.map50}\")\n",
    "print(f\"Mean Average Precision @ .70   : {metrics.box.map75}\")"
   ],
   "id": "e1a6e5f54340d27a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Model Prediction\n",
    "\n",
    "Now that we have a trained model, we can use it to make predictions. In this section, we will use the model to detect strategic fruits cards in new images. We will visualize the model's predictions by plotting the detected cards on the images."
   ],
   "id": "9389543f978fa3d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to perform detections with trained model\n",
    "def predict_detection(image_path):\n",
    "    \"\"\"\n",
    "    This function performs object detection on an image using a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    detect_image (ndarray): The image with detections plotted, in RGB format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Pass the image through the detection model and get the result\n",
    "    detect_result = model(image)\n",
    "\n",
    "    # Plot the detections\n",
    "    detect_image = detect_result[0].plot()\n",
    "\n",
    "    # Convert the image to RGB format\n",
    "    detect_image = cv2.cvtColor(detect_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return detect_image"
   ],
   "id": "d5e0a8ab079f3e4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Get list of all the image files in the test directory\n",
    "image_files = sorted(os.listdir(test_images))\n",
    "\n",
    "# Choose 12 random image files from the list\n",
    "sample_image_files = random.sample(image_files, 12)\n",
    "\n",
    "# Set up the plot\n",
    "fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
    "\n",
    "# Loop over the random images and plot the detections of the trained model\n",
    "for i, image_file in enumerate(sample_image_files):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "\n",
    "    # Load the current image and run object detection\n",
    "    image_path = os.path.join(test_images, image_file)\n",
    "    detect_image = predict_detection(image_path)\n",
    "\n",
    "    axs[row, col].imshow(detect_image)\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "id": "3e90d229547ebb2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
